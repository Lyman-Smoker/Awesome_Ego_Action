# Awesome Egocentric Action Understanding
Egocentric Action Understanding (EAU) aims at understanding human actions based on videos shot by first-person cameras.

In this reprository, interetsting papers in EAU are collected to show the development of the EAU community.

ðŸ’¥ **NEWS: ECCV-2024 papers are added to the list.**

### Survey
- A Survey on 3D Egocentric Human Pose Estimation **(CVPRW 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024W/Rhobin/html/Azam_A_Survey_on_3D_Egocentric_Human_Pose_Estimation_CVPRW_2024_paper.html)
- An Outlook into the Future of Egocentric Vision **(IJCV 2024)** [[Paper]](https://link.springer.com/article/10.1007/s11263-024-02095-7) [[Citations]](https://scholar.google.com/scholar?cites=15098752443409016265&as_sdt=2005&sciodt=0,5&hl=en)
 

### 2024
**Yearly Key words**: Ego-Pose, New Dataset, 3D, Ego-Exo, Multi-modality, HOI, Mistake Detection, Audio-Visual
- EMHI: A Multimodal Egocentric Human Motion Dataset with HMD and Body-Worn IMUs **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2408.17168)
- Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2408.14469) [[Project]](https://qirui-chen.github.io/MultiHop-EgoQA/) [[Code]](https://github.com/qirui-chen/MultiHop-EgoQA)
- Unlocking Exocentric Video-Language Data for Egocentric Video Representation Learning **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2408.03567)
- HUP-3D: A 3D multi-view synthetic dataset for assisted-egocentric hand-ultrasound pose estimation **(ArXiv 2024)** [[Paper]](https://arxiv.org/pdf/2407.09215)
- PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2407.09503) [[Project]](https://parse-ego4d.github.io)
- Egocentric Vision Language Planning **(ArXiv)** [[Paper]](https://arxiv.org/abs/2408.05802)
- EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D Gaussian Splatting **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2406.19811)
- EgoNCE++: Do Egocentric Video-Language Models Really Understand Hand-Object Interactions? **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2405.17719) [[Code]](https://github.com/xuboshen/EgoNCEpp)
- Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos **(ArXiv 2024)** [[Paper]](https://arxiv.org/pdf/2405.04370)
- EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2405.13659) [[Code]](https://github.com/yyvhang/EgoChoir_release) [[Project]](https://yyvhang.github.io/EgoChoir/)
- Spatial Cognition from Egocentric Video: Out of Sight, Not Out of Mind **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2404.05072) [[Project]](https://dimadamen.github.io/OSNOM/)
- EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric Foundation Models **(ArXiv)** [[Paper]](https://arxiv.org/pdf/2406.10224)
- HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2404.09933) [[Code]](https://github.com/Sid2697/HOI-Ref)
- Intention-driven Ego-to-Exo Video Generation **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2403.09194)
----
- Ego3DT: Tracking All 3D Objects in Ego-Centric Video of Daily Activities **(ACMMM 2024)**
- Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition **(ECCV 2024)** [[Paper]](https://masashi-hatano.github.io/assets/pdf/mm-cdfsl.pdf) [[Project]](https://masashi-hatano.github.io/MM-CDFSL/) [[Code]](https://github.com/masashi-hatano/MM-CDFSL)
- Masked Video and Body-worn IMU Autoencoder for Egocentric Action Recognition **(ECCV 2024)** [[Paper]](https://arxiv.org/pdf/2407.0662801)
- AMEGO: Active Memory from long EGOcentric videos **(ECCV 2024)**
- Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? **(ECCV 2024)**
- Discovering Novel Actions from Open World Egocentric Videos with Object-Grounded Visual Commonsense Reasoning **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2305.16602)
- Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2406.09272) [[Project]](https://vision.cs.utexas.edu/projects/action2sound/)
- Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2305.03907)
- Spherical World-Locking for Audio-Visual Localization in Egocentric Videos **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2408.05364) [[Project]](https://hs-yn.github.io/SWL/)
- LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning **(ECCV 2024)** [[Paper]](https://arxiv.org/pdf/2312.03849) [[Project]](https://bolinlai.github.io/Lego_EgoActGen/) [[Code]](https://github.com/BolinLai/LEGO)
- Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2403.16428)
- EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2308.06493)
- EgoPoseFormer: A Simple Baseline for Egocentric 3D Human Pose Estimation **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2403.18080)
- EgoBodu3M: Egocentric Body Tracking on a VR Headset using a Diverse Dataset **(ECCV 2024)**
- 3D Hand Pose Estimation in Everyday Egocentric Images **(ECCV 2024)** [[Paper]](https://ap229997.github.io/projects/hands/assets/paper.pdf) [[Project]](https://ap229997.github.io/projects/hands/)
- Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2403.06351) [[Citations]](https://scholar.google.com/scholar?cites=17491620542590999599&as_sdt=2005&sciodt=0,5&hl=en)
- EgoLifter: Open-world 3D Segmentation for Egocentric Perception **(ECCV 2024)** [[Project]](https://egolifter.github.io/#) [[Paper]](https://arxiv.org/abs/2403.18118)
- Nymeria: A Massive Collection of Multimodal Egocentric Daily Motion in the Wild **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2406.09905) [[Project]](https://www.projectaria.com/datasets/nymeria/)
- EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action Understanding **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2406.08877) [[Code]](https://github.com/iSEE-Laboratory/EgoExo-Fitness/tree/main)
- Ex2Eg-MAE: A Framework for Adaptation of Exocentric Video Masked Autoencoders for Egocentric Social Role Understanding **(ECCV 2024)**
- Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2312.02638) [[Code]](https://github.com/fpv-iplab/synchronization-is-all-you-need)
- EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video Retrieval **(ECCV 2024)** [[Paper]](https://arxiv.org/abs/2407.16658) [[Code]](https://github.com/ExplainableML/EgoCVR)
- SimpleEgo: Predicting Probabilistic Body Pose from Egocentric Cameras **(3DV 2024)** [[Paper]](https://arxiv.org/abs/2401.14785)
- Multi-Factor Adaptive Vision Selection for Egocentric Video Question Answering **(ICML 2024)** [[Paper]](https://openreview.net/pdf?id=u00dmbI8Db) [[Code]](https://github.com/Hyu-Zhang/EgoVideoQA)
- Grounded Question-Answering in Long Egocentric Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Di_Grounded_Question-Answering_in_Long_Egocentric_Videos_CVPR_2024_paper.html)
- Learning to Segment Referred Objects from Narrated Egocentric Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Shen_Learning_to_Segment_Referred_Objects_from_Narrated_Egocentric_Videos_CVPR_2024_paper.pdf)
- Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives **(CVPR 2024)** [[Project]](https://ego-exo4d-data.org/#people) [[Paper]](https://ego-exo4d-data.org/paper/ego-exo4d.pdf) [[Citations]](https://scholar.google.com/scholar?cites=6807725739327920288&as_sdt=2005&sciodt=0,5&hl=en)
- EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World **(CVPR 2024)** [[Paper]](https://arxiv.org/pdf/2403.16182.pdf) [[Code]](https://github.com/OpenGVLab/EgoExoLearn/)
- EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_EgoThink_Evaluating_First-Person_Perspective_Thinking_Capability_of_Vision-Language_Models_CVPR_2024_paper.html)
- The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Jia_The_Audio-Visual_Conversational_Graph_From_an_Egocentric-Exocentric_Perspective_CVPR_2024_paper.html)
- Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Majumder_Learning_Spatial_Features_from_Audio-Visual_Correspondence_in_Egocentric_Videos_CVPR_2024_paper.html)
- SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos **(CVPR 2024)**
- PREGO: online mistake detection in PRocedural EGOcentric videos **(CVPR 2024)** [[Paper]](https://arxiv.org/abs/2404.01933) [[Code]](https://github.com/aleflabo/PREGO)
- Error Detection in Egocentric Procedural Task Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.html) [[Code]](EgoExo_Fitness_CLIP_Vid_Feat_w_Rotate)
- 3D Human Pose Perception from Egocentric Stereo Videos **(CVPR 2024)**
- EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams **(CVPR 2024)** [[Project]](https://4dqv.mpi-inf.mpg.de/EventEgo3D/) [[Paper]](https://arxiv.org/pdf/2404.08640.pdf)
- Egocentric Whole-Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Egocentric_Whole-Body_Motion_Capture_with_FisheyeViT_and_Diffusion-Based_Motion_Refinement_CVPR_2024_paper.html)
- Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Kang_Attention-Propagation_Network_for_Egocentric_Heatmap_to_3D_Pose_Lifting_CVPR_2024_paper.html)
- Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Single-to-Dual-View_Adaptation_for_Egocentric_3D_Hand_Pose_Estimation_CVPR_2024_paper.html) [[Code]](https://github.com/ut-vision/S2DHand)
- Real-Time Simulated Avatar from Head-Mounted Sensors **(CVPR 2024)** [[Project]](https://zhengyiluo.github.io/SimXR/) [[Paper]](https://arxiv.org/abs/2403.06862)
- Instance Tracking in 3D Scenes from Egocentric Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Instance_Tracking_in_3D_Scenes_from_Egocentric_Videos_CVPR_2024_paper.html) [[Code]](https://github.com/IT3DEgo/IT3DEgo/)
- X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization **(CVPR 2024)** [[Code]](https://github.com/Annusha/xmic)
- A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives **(CVPR 2024)** [[Paper]](https://arxiv.org/pdf/2403.03037.pdf) [[Project]](https://sapeirone.github.io/EgoPack/)
- Progress-Aware Online Action Segmentation for Egocentric Procedural Task Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Shen_Progress-Aware_Online_Action_Segmentation_for_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.html) [[Code]](https://github.com/Yuhan-Shen/ProTAS)
- Action Scene Graphs for Long-Form Understanding of Egocentric Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Rodin_Action_Scene_Graphs_for_Long-Form_Understanding_of_Egocentric_Videos_CVPR_2024_paper.html) [[Code]](https://github.com/fpv-iplab/EASG)
- Retrieval-Augmented Egocentric Video Captioning **(CVPR 2024)** [[Paper]](https://arxiv.org/abs/2401.00789) [[Citations]](https://scholar.google.com/scholar?cites=17188265237658528377&as_sdt=2005&sciodt=0,5&hl=en) [[Project]](https://jazzcharles.github.io/Egoinstructor/) [[Code]](https://github.com/Jazzcharles/Egoinstructor/)
- OAKINK2 : A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion **(CVPR 2024)** [[Paper]](https://arxiv.org/pdf/2403.19417.pdf) [[Project]](https://oakink.net/v2/) [[Citations]](https://scholar.google.com/scholar?cites=7495043573208952724&as_sdt=2005&sciodt=0,5&hl=en)
- EgoGen: An Egocentric Synthetic Data Generator **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Li_EgoGen_An_Egocentric_Synthetic_Data_Generator_CVPR_2024_paper.html) [[Project]](https://ego-gen.github.io/) [[Code]](https://github.com/ligengen/EgoGen)
- Fusing Personal and Environmental Cues for Identification and Segmentation of First-Person Camera Wearers in Third-Person Views **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Fusing_Personal_and_Environmental_Cues_for_Identification_and_Segmentation_of_CVPR_2024_paper.html)

### 2023
- Ego-Body Pose Estimation via Ego-Head Pose Estimation **(CVPR 2023)** [[Project]](https://lijiaman.github.io/projects/egoego/) [[Paper]](http://openaccess.thecvf.com/content/CVPR2023/html/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2023_paper.html) [[Code]](https://github.com/lijiaman/egoego_release) [[Citations]](https://scholar.google.com/scholar?cites=4815197793392724124&as_sdt=2005&sciodt=0,5&hl=en)
- IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting **(WACV 2023)** [[Paper]](https://arxiv.org/pdf/2310.17323.pdf) [[Code]](https://github.com/TimSchoonbeek/IndustReal)
- EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone **(ICCV 2023)** [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Pramanick_EgoVLPv2_Egocentric_Video-Language_Pre-training_with_Fusion_in_the_Backbone_ICCV_2023_paper.html) [[Project]](https://shramanpramanick.github.io/EgoVLPv2/) [[Code]](https://github.com/facebookresearch/EgoVLPv2) [[Citations]](https://scholar.google.com/scholar?cites=2418402012858604493&as_sdt=2005&sciodt=0,5&hl=en)
- Weakly-Supervised Action Segmentation and Unseen Error Detection in Anomalous Instructional Videos **(ICCV 2023)** [[Project]](https://usa.honda-ri.com/ata) [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Ghoddoosian_Weakly-Supervised_Action_Segmentation_and_Unseen_Error_Detection_in_Anomalous_Instructional_ICCV_2023_paper.html) 
- HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World **(ICCV 2023)** [[Project]](https://holoassist.github.io) [[Paper]](http://openaccess.thecvf.com/content/ICCV2023/html/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.html) [[Citations]](https://scholar.google.com/scholar?cites=1935067381543829055&as_sdt=2005&sciodt=0,5&hl=en) [[Citations]](https://scholar.google.com/scholar?cites=1935067381543829055&as_sdt=2005&sciodt=0,5&hl=en)
- Ego-Humans: An Ego-Centric 3D Multi-Human Benchmark **(ICCV 2023)** [[Paper]](http://openaccess.thecvf.com/content/ICCV2023/html/Khirodkar_Ego-Humans_An_Ego-Centric_3D_Multi-Human_Benchmark_ICCV_2023_paper.html) [[Code]](https://github.com/rawalkhirodkar/egohumans) [[Citations]](https://scholar.google.com/scholar?cites=17892059476155692076&as_sdt=2005&sciodt=0,5&hl=en)
- CaptainCook4D: A dataset for understanding errors in procedural activities **(ICMLW 2023)** [[Project]](https://captaincook4d.github.io/captain-cook/) [[Paper]](https://arxiv.org/abs/2312.14556) [[Code]](https://github.com/CaptainCook4D)
- Every Mistake Counts in Assembly **(ArXiv 2023)** [[Paper]](https://arxiv.org/abs/2307.16453) [[Code]](https://github.com/assembly-101/assembly101-mistake-detection)

### 2022
- Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities **(CVPR 2022)** [[Project]](https://assembly-101.github.io) [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Sener_Assembly101_A_Large-Scale_Multi-View_Video_Dataset_for_Understanding_Procedural_Activities_CVPR_2022_paper.pdf) [[Code]](https://github.com/assembly-101?tab=repositories) [[Citations]](https://scholar.google.com/scholar?cites=16985062727042180828&as_sdt=2005&sciodt=0,5&hl=en)
