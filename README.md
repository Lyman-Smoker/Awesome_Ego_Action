# Awesome Egocentric Action Understanding
Egocentric Action Understanding (EAU) aims at understanding human actions based on videos shot by first-person cameras.

In this reprository, interetsting papers in EAU are collected to show the development of the EAU community.

### Survey
- A Survey on 3D Egocentric Human Pose Estimation **(2024 ArXiv)** [[Paper]](https://arxiv.org/abs/2403.17893)
- An Outlook into the Future of Egocentric Vision **(2023 ArXiv)** [[Paper]](https://arxiv.org/pdf/2308.07123.pdf) [[Citations]](https://scholar.google.com/scholar?cites=15098752443409016265&as_sdt=2005&sciodt=0,5&hl=en)
 

### 2024
- EgoNCE++: Do Egocentric Video-Language Models Really Understand Hand-Object Interactions? **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2405.17719) [[Code]](https://github.com/xuboshen/EgoNCEpp)
- Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2405.04370) [[Citations]](https://scholar.google.com/scholar?cites=17491620542590999599&as_sdt=2005&sciodt=0,5&hl=en)
- Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos **(ArXiv 2024)** [[Paper]](https://arxiv.org/pdf/2405.04370)
- EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2405.13659)
- Spatial Cognition from Egocentric Video: Out of Sight, Not Out of Mind **(ArXiv 2024)**
- EgoLifter: Open-world 3D Segmentation for Egocentric Perception **(ArXiv 2024)** [[Project]](https://egolifter.github.io/#) [[Paper]](https://arxiv.org/abs/2403.18118)
- Intention-driven Ego-to-Exo Video Generation **(ArXiv 2024)** [[Paper]](https://arxiv.org/abs/2403.09194)
- Grounded Question-Answering in Long Egocentric Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Di_Grounded_Question-Answering_in_Long_Egocentric_Videos_CVPR_2024_paper.html)
- Learning to Segment Referred Objects from Narrated Egocentric Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Shen_Learning_to_Segment_Referred_Objects_from_Narrated_Egocentric_Videos_CVPR_2024_paper.pdf)
- Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives **(CVPR 2024)** [[Project]](https://ego-exo4d-data.org/#people) [[Paper]](https://ego-exo4d-data.org/paper/ego-exo4d.pdf) [[Citations]](https://scholar.google.com/scholar?cites=6807725739327920288&as_sdt=2005&sciodt=0,5&hl=en)
- EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World **(CVPR 2024)** [[Paper]](https://arxiv.org/pdf/2403.16182.pdf) [[Code]](https://github.com/OpenGVLab/EgoExoLearn/)
- The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Jia_The_Audio-Visual_Conversational_Graph_From_an_Egocentric-Exocentric_Perspective_CVPR_2024_paper.html)
- Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Majumder_Learning_Spatial_Features_from_Audio-Visual_Correspondence_in_Egocentric_Videos_CVPR_2024_paper.html)
- SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos **(CVPR 2024)**
- PREGO: online mistake detection in PRocedural EGOcentric videos **(CVPR 2024)** [[Paper]](https://arxiv.org/abs/2404.01933) [[Code]](https://github.com/aleflabo/PREGO)
- Error Detection in Egocentric Procedural Task Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Lee_Error_Detection_in_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.html)
- 3D Human Pose Perception from Egocentric Stereo Videos **(CVPR 2024)**
- EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams **(CVPR 2024)** [[Project]](https://4dqv.mpi-inf.mpg.de/EventEgo3D/) [[Paper]](https://arxiv.org/pdf/2404.08640.pdf)
- Egocentric Whole-Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Egocentric_Whole-Body_Motion_Capture_with_FisheyeViT_and_Diffusion-Based_Motion_Refinement_CVPR_2024_paper.html)
- Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Kang_Attention-Propagation_Network_for_Egocentric_Heatmap_to_3D_Pose_Lifting_CVPR_2024_paper.html)
- Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Single-to-Dual-View_Adaptation_for_Egocentric_3D_Hand_Pose_Estimation_CVPR_2024_paper.html)
- Real-Time Simulated Avatar from Head-Mounted Sensors **(CVPR 2024)** [[Project]](https://zhengyiluo.github.io/SimXR/) [[Paper]](https://arxiv.org/abs/2403.06862)
- Instance Tracking in 3D Scenes from Egocentric Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Instance_Tracking_in_3D_Scenes_from_Egocentric_Videos_CVPR_2024_paper.html)
- X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization **(CVPR 2024)** [[Code]](https://github.com/Annusha/xmic)
- A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives **(CVPR 2024)** [[Paper]](https://arxiv.org/pdf/2403.03037.pdf) [[Project]](https://sapeirone.github.io/EgoPack/)
- Progress-Aware Online Action Segmentation for Egocentric Procedural Task Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Shen_Progress-Aware_Online_Action_Segmentation_for_Egocentric_Procedural_Task_Videos_CVPR_2024_paper.html)
- Action Scene Graphs for Long-Form Understanding of Egocentric Videos **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Rodin_Action_Scene_Graphs_for_Long-Form_Understanding_of_Egocentric_Videos_CVPR_2024_paper.html)
- Retrieval-Augmented Egocentric Video Captioning **(CVPR 2024)** [[Paper]](https://arxiv.org/abs/2401.00789) [[Citations]](https://scholar.google.com/scholar?cites=17188265237658528377&as_sdt=2005&sciodt=0,5&hl=en)
- OAKINK2 : A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion **(CVPR 2024)** [[Paper]](https://arxiv.org/pdf/2403.19417.pdf) [[Project]](https://oakink.net/v2/)
- EgoGen: An Egocentric Synthetic Data Generator **(CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Li_EgoGen_An_Egocentric_Synthetic_Data_Generator_CVPR_2024_paper.html)

### 2023
- Ego-Body Pose Estimation via Ego-Head Pose Estimation **(CVPR 2023)** [[Project]](https://lijiaman.github.io/projects/egoego/) [[Paper]](http://openaccess.thecvf.com/content/CVPR2023/html/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2023_paper.html) [[Code]](https://github.com/lijiaman/egoego_release) [[Citations]](https://scholar.google.com/scholar?cites=4815197793392724124&as_sdt=2005&sciodt=0,5&hl=en)
- IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting **(WACV 2023)** [[Paper]](https://arxiv.org/pdf/2310.17323.pdf) [[Code]](https://github.com/TimSchoonbeek/IndustReal)
- EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone **(ICCV 2023)** [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Pramanick_EgoVLPv2_Egocentric_Video-Language_Pre-training_with_Fusion_in_the_Backbone_ICCV_2023_paper.html) [[Project]](https://shramanpramanick.github.io/EgoVLPv2/) [[Code]](https://github.com/facebookresearch/EgoVLPv2) [[Citations]](https://scholar.google.com/scholar?cites=2418402012858604493&as_sdt=2005&sciodt=0,5&hl=en)
- Weakly-Supervised Action Segmentation and Unseen Error Detection in Anomalous Instructional Videos **(ICCV 2023)** [[Project]](https://usa.honda-ri.com/ata) [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/html/Ghoddoosian_Weakly-Supervised_Action_Segmentation_and_Unseen_Error_Detection_in_Anomalous_Instructional_ICCV_2023_paper.html) 
- HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World **(ICCV 2023)** [[Project]](https://holoassist.github.io) [[Paper]](http://openaccess.thecvf.com/content/ICCV2023/html/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.html) [[Citations]](https://scholar.google.com/scholar?cites=1935067381543829055&as_sdt=2005&sciodt=0,5&hl=en) [[Citations]](https://scholar.google.com/scholar?cites=1935067381543829055&as_sdt=2005&sciodt=0,5&hl=en)
- Ego-Humans: An Ego-Centric 3D Multi-Human Benchmark [[Paper]](http://openaccess.thecvf.com/content/ICCV2023/html/Khirodkar_Ego-Humans_An_Ego-Centric_3D_Multi-Human_Benchmark_ICCV_2023_paper.html) [[Code]](https://github.com/rawalkhirodkar/egohumans) [[Citations]](https://scholar.google.com/scholar?cites=17892059476155692076&as_sdt=2005&sciodt=0,5&hl=en)
- CaptainCook4D: A dataset for understanding errors in procedural activities **(ICMLW 2023)** [[Project]](https://captaincook4d.github.io/captain-cook/) [[Paper]](https://arxiv.org/abs/2312.14556) [[Code]](https://github.com/CaptainCook4D)
- Every Mistake Counts in Assembly **(ArXiv 2023)** [[Paper]](https://arxiv.org/abs/2307.16453) [[Code]](https://github.com/assembly-101/assembly101-mistake-detection)

### 2022
- Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities **(CVPR 2022)** [[Project]](https://assembly-101.github.io) [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Sener_Assembly101_A_Large-Scale_Multi-View_Video_Dataset_for_Understanding_Procedural_Activities_CVPR_2022_paper.pdf) [[Code]](https://github.com/assembly-101?tab=repositories) [[Citations]](https://scholar.google.com/scholar?cites=16985062727042180828&as_sdt=2005&sciodt=0,5&hl=en)
